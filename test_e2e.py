#!/usr/bin/env python3
"""
Comprehensive End-to-End Test Suite for jesse-mcp
Tests all 17 tools with realistic workflows and integration scenarios
"""

import asyncio
import sys
import os
import time
from datetime import datetime

# Add current directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from server import JesseMCPServer

    print("‚úÖ Jesse MCP Server imported successfully")
except ImportError as e:
    print(f"‚ùå Failed to import server: {e}")
    sys.exit(1)


class E2ETestSuite:
    """Comprehensive end-to-end test suite"""

    def __init__(self):
        self.server = JesseMCPServer()
        self.results = []
        self.start_time = time.time()

    async def run_all_tests(self) -> bool:
        """Run all E2E tests"""
        print("\n" + "=" * 70)
        print("üß™ JESSE-MCP END-TO-END TEST SUITE")
        print("=" * 70)
        print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # Test categories
        await self._test_tool_availability()
        await self._test_individual_tools()
        await self._test_tool_chains()
        await self._test_mcp_routing()
        await self._test_error_handling()
        await self._test_performance()

        # Print summary
        return self._print_summary()

    async def _test_tool_availability(self):
        """Test 1: Verify all 17 tools are available"""
        print("TEST 1: Tool Availability")
        print("-" * 70)

        try:
            tools = await self.server.list_tools()
            total_tools = len(tools)
            tool_names = [t["name"] for t in tools]

            expected_tools = {
                "backtest",
                "strategy_list",
                "strategy_read",
                "strategy_validate",
                "candles_import",
                "backtest_batch",
                "analyze_results",
                "walk_forward",
                "optimize",
                "monte_carlo",
                "var_calculation",
                "stress_test",
                "risk_report",
                "correlation_matrix",
                "pairs_backtest",
                "factor_analysis",
                "regime_detector",
            }

            available = set(tool_names)
            missing = expected_tools - available
            extra = available - expected_tools

            print(f"  Total tools: {total_tools}")
            print(f"  Expected: {len(expected_tools)}")
            print(f"  Available: {len(available)}")

            if missing:
                print(f"  ‚ùå Missing tools: {missing}")
                self.results.append(("Tool Availability", False))
                return

            if extra:
                print(f"  ‚ö†Ô∏è  Extra tools: {extra}")

            success = len(available) == 17
            status = "‚úÖ PASS" if success else "‚ùå FAIL"
            print(f"\n  {status}: All 17 tools available")
            self.results.append(("Tool Availability", success))

        except Exception as e:
            print(f"  ‚ùå FAIL: {e}")
            self.results.append(("Tool Availability", False))

    async def _test_individual_tools(self):
        """Test 2: Test each tool individually"""
        print("\nTEST 2: Individual Tool Functionality")
        print("-" * 70)

        # Mock backtest result with full equity curve
        mock_result = {
            "strategy": "TestStrategy",
            "symbol": "BTC-USDT",
            "timeframe": "1h",
            "start_date": "2024-01-01",
            "end_date": "2024-02-01",
            "total_return": 0.15,
            "sharpe_ratio": 1.2,
            "max_drawdown": 0.1,
            "win_rate": 0.55,
            "total_trades": 50,
            "equity_curve": [
                {"date": "2024-01-01", "return": 0.01, "equity": 10010},
                {"date": "2024-01-02", "return": -0.02, "equity": 9980},
                {"date": "2024-01-03", "return": 0.03, "equity": 10010},
                {"date": "2024-01-04", "return": 0.01, "equity": 10040},
                {"date": "2024-01-05", "return": 0.02, "equity": 10060},
                {"date": "2024-01-06", "return": -0.01, "equity": 10050},
                {"date": "2024-01-07", "return": 0.015, "equity": 10100},
            ],
        }

        # Test Phase 1-2 mock tools
        phase1_tools = ["strategy_list", "strategy_validate"]
        phase4_tools = [
            "monte_carlo",
            "var_calculation",
            "stress_test",
            "risk_report",
        ]
        phase5_tools = [
            "correlation_matrix",
            "pairs_backtest",
            "factor_analysis",
            "regime_detector",
        ]

        test_configs = {
            "strategy_list": {},
            "strategy_validate": {"code": "pass"},
            "monte_carlo": {
                "backtest_result": mock_result,
                "simulations": 100,
            },
            "var_calculation": {"backtest_result": mock_result},
            "stress_test": {"backtest_result": mock_result},
            "risk_report": {"backtest_result": mock_result},
            "correlation_matrix": {"backtest_results": [mock_result, mock_result]},
            "pairs_backtest": {
                "pair": {"symbol1": "BTC-USDT", "symbol2": "ETH-USDT"},
                "backtest_result_1": mock_result,
                "backtest_result_2": mock_result,
            },
            "factor_analysis": {"backtest_result": mock_result},
            "regime_detector": {"backtest_results": [mock_result]},
        }

        passed = 0
        failed = 0

        for tool_name, args in test_configs.items():
            try:
                result = await self.server.call_tool(tool_name, args)

                if "error" in result:
                    # Jesse-specific tools will fail without Jesse - that's expected
                    if tool_name in ["strategy_list", "strategy_validate"]:
                        print(
                            f"  ‚ö†Ô∏è  {tool_name}: {result['error']} (expected - Jesse required)"
                        )
                        passed += 1  # Count as pass since it's expected
                    else:
                        print(f"  ‚ùå {tool_name}: {result['error']}")
                        failed += 1
                else:
                    print(f"  ‚úÖ {tool_name}")
                    passed += 1

            except Exception as e:
                print(f"  ‚ùå {tool_name}: {e}")
                failed += 1

        print(f"\n  Result: {passed}/{passed + failed} tools working")
        self.results.append(("Individual Tools", passed == len(test_configs)))

    async def _test_tool_chains(self):
        """Test 3: Test tool chains (combining multiple tools)"""
        print("\nTEST 3: Tool Chains & Workflows")
        print("-" * 70)

        mock_result = {
            "strategy": "TestStrategy",
            "symbol": "BTC-USDT",
            "timeframe": "1h",
            "start_date": "2024-01-01",
            "end_date": "2024-02-01",
            "total_return": 0.15,
            "sharpe_ratio": 1.2,
            "max_drawdown": 0.1,
            "win_rate": 0.55,
            "total_trades": 50,
            "equity_curve": [
                {"date": "2024-01-01", "return": 0.01, "equity": 10010},
                {"date": "2024-01-02", "return": -0.02, "equity": 9980},
                {"date": "2024-01-03", "return": 0.03, "equity": 10010},
                {"date": "2024-01-04", "return": 0.01, "equity": 10040},
            ],
        }

        chains = [
            {
                "name": "Backtest ‚Üí Risk Analysis ‚Üí Report",
                "tools": [
                    ("stress_test", {"backtest_result": mock_result}),
                    ("risk_report", {"backtest_result": mock_result}),
                ],
            },
            {
                "name": "Multi-Asset ‚Üí Correlation ‚Üí Pairs",
                "tools": [
                    (
                        "correlation_matrix",
                        {
                            "backtest_results": [
                                {**mock_result, "symbol": "BTC-USDT"},
                                {**mock_result, "symbol": "ETH-USDT"},
                            ]
                        },
                    ),
                    (
                        "pairs_backtest",
                        {
                            "pair": {"symbol1": "BTC-USDT", "symbol2": "ETH-USDT"},
                            "backtest_result_1": mock_result,
                            "backtest_result_2": mock_result,
                        },
                    ),
                ],
            },
            {
                "name": "Analysis ‚Üí Factor Decomposition ‚Üí Regime",
                "tools": [
                    ("factor_analysis", {"backtest_result": mock_result}),
                    ("regime_detector", {"backtest_results": [mock_result]}),
                ],
            },
        ]

        passed = 0

        for chain in chains:
            try:
                for tool_name, args in chain["tools"]:
                    result = await self.server.call_tool(tool_name, args)
                    if "error" in result:
                        raise Exception(f"{tool_name} failed: {result['error']}")

                print(f"  ‚úÖ {chain['name']}")
                passed += 1

            except Exception as e:
                print(f"  ‚ùå {chain['name']}: {e}")

        print(f"\n  Result: {passed}/{len(chains)} chains working")
        self.results.append(("Tool Chains", passed == len(chains)))

    async def _test_mcp_routing(self):
        """Test 4: MCP protocol routing and integration"""
        print("\nTEST 4: MCP Server Routing & Integration")
        print("-" * 70)

        mock_result = {
            "strategy": "TestStrategy",
            "symbol": "BTC-USDT",
            "total_return": 0.15,
            "sharpe_ratio": 1.2,
            "equity_curve": [
                {"date": "2024-01-01", "return": 0.01, "equity": 10010},
                {"date": "2024-01-02", "return": -0.02, "equity": 9980},
                {"date": "2024-01-03", "return": 0.03, "equity": 10010},
            ],
        }

        test_calls = [
            ("tools/list", {}),
            ("tools/call", {"name": "strategy_list", "arguments": {}}),
            (
                "tools/call",
                {
                    "name": "monte_carlo",
                    "arguments": {"backtest_result": mock_result, "simulations": 50},
                },
            ),
        ]

        passed = 0

        for method, params in test_calls:
            try:
                # Simulate MCP request handling
                if method == "tools/list":
                    tools = await self.server.list_tools()
                    if len(tools) == 17:
                        print(f"  ‚úÖ {method}: {len(tools)} tools listed")
                        passed += 1
                    else:
                        print(f"  ‚ùå {method}: Expected 17 tools, got {len(tools)}")

                elif method == "tools/call":
                    result = await self.server.call_tool(
                        params["name"], params["arguments"]
                    )
                    tool_name = params["name"]
                    if "error" not in result or result.get("error") is None:
                        print(f"  ‚úÖ {method}: {tool_name}")
                        passed += 1
                    else:
                        # Jesse-specific tools will fail without Jesse - that's expected
                        if tool_name in ["strategy_list", "strategy_validate"]:
                            print(
                                f"  ‚ö†Ô∏è  {method}: {tool_name} - {result['error']} (expected)"
                            )
                            passed += 1
                        else:
                            print(f"  ‚ùå {method}: {tool_name} - {result['error']}")

            except Exception as e:
                print(f"  ‚ùå {method}: {e}")

        print(f"\n  Result: {passed}/{len(test_calls)} routing tests passed")
        self.results.append(("MCP Routing", passed == len(test_calls)))

    async def _test_error_handling(self):
        """Test 5: Error handling and edge cases"""
        print("\nTEST 5: Error Handling & Edge Cases")
        print("-" * 70)

        error_tests = [
            {
                "name": "Unknown tool",
                "tool": "nonexistent_tool",
                "args": {},
                "expect_error": True,
            },
            {
                "name": "Missing required args",
                "tool": "pairs_backtest",
                "args": {},
                "expect_error": True,
            },
            {
                "name": "Invalid tool arguments",
                "tool": "monte_carlo",
                "args": {"backtest_result": None},
                "expect_error": True,
            },
            {
                "name": "Empty backtest results",
                "tool": "correlation_matrix",
                "args": {"backtest_results": []},
                "expect_error": False,  # Should handle gracefully
            },
        ]

        passed = 0

        for test in error_tests:
            try:
                result = await self.server.call_tool(test["tool"], test["args"])
                has_error = "error" in result and result["error"] is not None

                if test["expect_error"] and has_error:
                    print(f"  ‚úÖ {test['name']}: Correctly reported error")
                    passed += 1
                elif not test["expect_error"] and not has_error:
                    print(f"  ‚úÖ {test['name']}: Handled gracefully")
                    passed += 1
                elif test["expect_error"] and not has_error:
                    print(f"  ‚ö†Ô∏è  {test['name']}: Expected error but succeeded")
                else:
                    print(
                        f"  ‚ùå {test['name']}: {result.get('error', 'Unknown error')}"
                    )

            except Exception as e:
                if test["expect_error"]:
                    print(f"  ‚úÖ {test['name']}: Correctly raised exception")
                    passed += 1
                else:
                    print(f"  ‚ùå {test['name']}: Unexpected exception - {e}")

        print(f"\n  Result: {passed}/{len(error_tests)} error handling tests passed")
        self.results.append(("Error Handling", passed >= len(error_tests) - 1))

    async def _test_performance(self):
        """Test 6: Performance and stress testing"""
        print("\nTEST 6: Performance & Stress Testing")
        print("-" * 70)

        mock_result = {
            "strategy": "TestStrategy",
            "symbol": "BTC-USDT",
            "total_return": 0.15,
            "sharpe_ratio": 1.2,
        }

        # Test response time
        print("  Performance Metrics:")

        # 1. Tool listing speed
        start = time.time()
        tools = await self.server.list_tools()
        list_time = time.time() - start
        print(f"    - Tool listing: {list_time * 1000:.2f}ms")

        # 2. Individual tool speed
        start = time.time()
        await self.server.call_tool("strategy_list", {})
        tool_time = time.time() - start
        print(f"    - Tool execution: {tool_time * 1000:.2f}ms")

        # 3. Concurrent tool calls
        start = time.time()
        tasks = [
            self.server.call_tool(
                "monte_carlo", {"backtest_result": mock_result, "simulations": 50}
            ),
            self.server.call_tool("risk_report", {"backtest_result": mock_result}),
            self.server.call_tool(
                "correlation_matrix", {"backtest_results": [mock_result]}
            ),
        ]
        results = await asyncio.gather(*tasks)
        concurrent_time = time.time() - start
        print(f"    - 3 concurrent tools: {concurrent_time * 1000:.2f}ms")

        # Check performance thresholds
        perf_ok = list_time < 0.1 and tool_time < 0.5 and concurrent_time < 2.0
        status = "‚úÖ PASS" if perf_ok else "‚ö†Ô∏è  WARNING"
        print(f"\n  {status}: Performance within acceptable ranges")

        self.results.append(("Performance", perf_ok))

    def _print_summary(self) -> bool:
        """Print test summary and return overall status"""
        print("\n" + "=" * 70)
        print("üìã TEST SUMMARY")
        print("=" * 70)

        passed = sum(1 for _, success in self.results if success)
        total = len(self.results)

        for test_name, success in self.results:
            status = "‚úÖ PASS" if success else "‚ùå FAIL"
            print(f"  {test_name:.<40} {status}")

        elapsed = time.time() - self.start_time
        print(f"\n  Total elapsed time: {elapsed:.2f}s")
        print(f"  Overall result: {passed}/{total} test categories passed")

        if passed == total:
            print("\nüéâ ALL TESTS PASSED! System is production-ready!")
            return True
        else:
            print(f"\n‚ö†Ô∏è  {total - passed} test category(ies) need attention")
            return False


async def main():
    """Run the E2E test suite"""
    suite = E2ETestSuite()
    success = await suite.run_all_tests()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())
